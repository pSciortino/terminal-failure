---
title: "Forecasting payment terminals failure"
author: "Patrick"
date: "1/19/2020"
output:
  pdf_document:
    # toc: yes
    # toc_depth: '2'
  html_document:
    collapse: no
    df_print: paged
    toc: yes
    toc_depth: 3
    toc_float: yes
bibliography: bibliography.bibtex
---

```{r setup, include=FALSE, message = FALSE}
library(tidyverse)
library(formattable)
library(DT)
library(kernhaz)
library(survPresmooth)
library(lubridate)
library(ggpubr)
library(tsbox)
library(modelr)
library(kableExtra)
```

```{r, include = FALSE}
### Sales parameters ---------
sales_A <- 3000
startdate_A <- as.Date("2017/1/1")
enddate_A <- as.Date("2019/12/31")

sales_B <- 2000
startdate_B <- as.Date("2018/1/1")
enddate_B <- as.Date("2019/12/31")

### Device parts parameters ---------

parts_A <- c("K01", "S01")
parts_B <- c("K02", "S01")
```

## Project summary

Say the company we work for manufactures and sells electronic devices in a B2B context while offering after sales services. In some sectors, shutdown time might be extremely costly for clients and handling device reparations requires maintening a sufficient stock of spare parts at hand. Yet, producing and storing those parts comes with a cost which is why inventory levels should be about just high enough to cover the demand. This project explores the idea of quantifying and forecasting the occurence of device failures and therefore the future reparation needs using as input historical data on product sales and reparations (as a proxy for failure occurences).

Literature reviews on the topic [@van2019forecasting; @dekker2013use] stress the poor performence of traditional time series forecasting methods for the task of predicting spare parts demand. A first challenge is that the occurence of failures is erratic and intermitent. Another is that our expectations on the number of failures should obviously take into account the number of devices currently on the market, which we will refer to as the *installed base*, as well as its expected evolution, rather than naÃ¯vely extrapolating patterns observed in historical failure data. 

The method explored here consists in three steps: First, a time-to-failure probability distribution will be estimated for each device part and failure type based on historical failure data. In a second phase, a forecast of future failures is performed based on the current state of the installed base by exploiting the fact that the expected number of part failures in time t is the sum of the individual probabilities to fail in time t for all parts in service. Finally, the exercise will be repeated in a setting where the future size and composition of the installed base is estimated.

<!-- ### Literature takeaways -->

<!-- The proposed method follows the literature's recommendations to think of the installed base as the driving factor for failures and therefore to exploit its current but also future state for the purpose of building expectations on future failures. Besides, it follows the subset of this literature that estimates time-to-failure probability distributions for each particular product instead of single-point estimates. What distinguishes the present attempt is the use of a non-parametric density estimation technique to build these probability distributions while generally a specific distribution is assumed. I also explore how statistical information can be combined with business knowledge to build pragmatic expectations on the future evolution of the installed base. -->

## Data
```{r, echo = FALSE}
source_df <- read.csv("source_df.csv")
```

The data used as source for the analysis is generated for the purpose of this project (see `Dataset generation: commented code`). The minimum requirement for this methodology to work is that one can gather the following information for (most) devices that have been sold in the past:

* its model;
* the date at which it has been sold;
* the reparation date, if any;
* the description of the failure (i.e. part that failed and reason), if any.

The dataset used here contains `r nrow(source_df)` records of that type (see first 5 records on Table 1). In total, `r sales_A` devices of model `A` were sold between `r format(startdate_A, "%B %d %Y")` and `r format(enddate_A, "%B %d %Y")` and `r sales_B` devices of model `B` were sold between `r format(startdate_B, "%B %d %Y")` and `r format(enddate_B, "%B %d %Y")`. In total, `r length(unique(source_df$failure_description))` different types of failures were recorded with the following frequencies listed in Table 2.
```{r, echo = FALSE}
source_df %>%
  head(5) %>%
  kable("latex", booktabs = T, caption = "Dataset") %>%
  kable_styling(position = "center", latex_options = "HOLD_position")
```



```{r, echo = FALSE, warning=FALSE,message=FALSE}
source_df %>%
  group_by(failure_description) %>%
  summarize(n = n()) %>%
  kable("latex", booktabs = T, caption = "Failures frequency") %>%
  kable_styling(position = "center", latex_options = "HOLD_position")
```


## Exploration and density estimation

Now let's start exploring the dataset as if we didn't know its generating process. First we'd be interested in understanding the lifetime at which failures generally occur. A first look reveals that the typical time-to-failure is quite different from part to part, hence from device to device.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
source_df$date_sold <- as.Date(source_df$date_sold)
source_df$date_repaired <- as.Date(source_df$date_repaired)

source_df <- source_df %>%
  mutate(time_to_failure = as.numeric(difftime(ymd(date_repaired), ymd(date_sold), units = "days"))) %>%
  mutate(
    part = ifelse(str_detect(part_code,"K"), "Keyboard", "Screen")
  )

(p0 <- source_df %>%
  ggplot(aes(time_to_failure)) + geom_histogram(aes(fill=failure_description, color = failure_description), binwidth = 5) +
  facet_wrap(~part_code, ncol = 1) +
  theme_light()  + labs(y = "frequency", x = "time-to-failure (days)", title = "Time-to-failure by part and by failure description") +
  ylim(0,15))
```

We already see some interesting patterns here, for example a lot of failures are recorded in the first 50 days of life for the part *K01*, while failure seems more spead for other parts, but it's quite hard to tell more at this point. Besides, such a histogram suffers from a bias due to the right-censored nature of time-to-failure data. Suppose the vast majority of the devices containing the part *K01* have been sold in the last 50 days, then the relative peak we observe on the figure might simply reflect the fact that most of the devices about which we know the reparation date are relatively young.

A way to make better use of these data is to estimate probability density functions of time-to-failure for each device part. The obtained density functions will have three advantages:

* get a smooth distribution of lifetimes at which failures are likely to occur by getting rid of uninformative spikes;
* get a vector of (relative) failure probabilities instead of frequencies. This is what we'll need for the forecasting part.
* provided that a proper technique is used, get rid of the censoring bias.

We'll use a version of the Kernel density estimator introduced by @foldes1981strong in which kernels are weighted so as to make it robust to right-censoring. The R function `presmooth` implements that method.

What I want to get for each pair (*part*; *failure type*) is a probability distribution of the pair's time-to-failure.
```{r, echo = FALSE, warning=FALSE, message=FALSE}

# 1. Adapt the dataset for the presmooth function that takes as parameter two vector: one of (censored) durations 
# and one of a censoring indicator. Censored values must be coded as 0, uncensored values as 1.
source_df2 <- source_df %>%
  mutate(time_to_failure = ifelse(is.na(time_to_failure), difftime(Sys.Date(), date_sold, units = "days"), time_to_failure)) %>%
  select(-c(date_sold, date_repaired, terminal_model, X, part))

source_df_nested <- source_df2 %>%
  group_by(part_code, failure_description) %>%
  nest()

get_censored_format <- function(part, description){
  models_containing_that_part <-  source_df %>%
    filter(part_code == part) %>%
    select(terminal_model) %>%
    unique()
  
  df <- source_df %>%
    unnest() %>%
    filter(terminal_model %in% models_containing_that_part) %>%
    mutate(censoring_indicator = ifelse(failure_description == description, 1, 0)) %>%
    mutate(failure_description = description) %>%
    select(failure_description, time_to_failure, censoring_indicator)
  
  return(df)
}

ready_df <- source_df_nested %>%
  mutate(censoring_info = map2(part_code, failure_description, get_censored_format)) 


# 2. Nest dataset so that each value in the column "data" is actually a 2-columns dataframe that records for each failure 
# the lifetime at which it occured and the total number of devices from the corresponding model in the installed base.
source_df_censored_nested <- source_df_censored %>%
  select(-terminal_model) %>%
  group_by(part, part_code, failure_description) %>%
  filter(!is.na(failure_description)) %>% ### This is the issue: I cannot drop all these lines, instead I should reattribute (part;failure_description) labels in the right (empirical) proportion to empty rows 
  nest()

# 3. Create function that returns the estimated probability density function
get_weighted_presmooth_density_df  <- function(df) {
  
  kdensity <- presmooth(times = df[["time_to_failure"]], status = df[["censoring_indicator"]], estimand = "f", x.est = seq(0,3000, by=1), bw.selec = "plug-in")
  data.frame(
    life_length = kdensity$x.est,
    weighted_density = kdensity$estimate
  )
}

# Now we'll have to find a way to attribute failure_description's to empty rows 
# in proportions that correspond to their relative likelihood
# i.e. their relative frequency + their distribution

# Maybe for a given device i, duplicating empty rows for each of its parts and then consider these observations as censored might work. It would actually do the weighting down process at the same time as handling the right-censoring bias.

# 4. Apply function
# densities_df <- source_df_censored_nested %>%
#   mutate(density_est = map(data, get_weighted_presmooth_density_df)) %>%
#   select(-data)
```




## References

