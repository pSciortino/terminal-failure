---
title: "Forecasting payment terminals failure"
output:
  html_document:
    collapse: no
    df_print: paged
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---
```{r setup, include=FALSE}
library(tidyverse)
library(formattable)
library(DT)
```

Say your company manufactures and sells electronic devices while offering a guarantee on each product. This project explores the idea of quantifying and forecasting the occurence of such defaults and therefore the future reparation/replacement needs using as input historical data on reparation and sales.

## Dataset

```{r, include = FALSE}
# Parameters ------------------------------------------------------------

### Defaults dataset generation ---------

# terminals of model A 
nr_a = 2000
failure_rate_a = 0.80
parts_a <- c("K01", "S01")

# terminals of model B
nr_b = 1000
failure_rate_b = 0.75
parts_b <- c("K02", "S01")


### Sales dataset generation ---------
startdate <- as.Date("2015/1/1")
enddate <- as.Date("2019/12/1")
sales_A <- 100
sales_B <- 200
```


```{r, include = FALSE}
source("Generate-data.R")
```

The two datasets used as source for the analysis are generated for the purpose of this project.

### Reparation data
In total, `r nr_a` terminals of model A have been sold and `r formattable::percent(failure_rate_a,0)` were returned due to failure. More specifically, x were returned because of a screen failure and y because of a keyboard failure. (to be continued)

We get:
```{r, echo=FALSE, warning =FALSE}
failures_df %>%
  datatable()
```

### Sales data

Say `r sales_A` devices of model A and `r sales_B` devices of model B were sold between `r format(startdate, "%B %d %Y")` and `r format(enddate, "%B %d %Y")`,  where the date at which an individual device has been sold is randomly drawn from this time range. We get:
```{r}
sales_df
```


## Exploration and density estimation

Now let's start exploring the reparation dataset as if we didn't know the generation process. First we're interested in understanding the lifetime at which failures generally occur. Here we visualize the frequency of the devices lifetime at failure for 3 failures type and 3 different device parts.
```{r, echo=FALSE, warning=FALSE}
(p0 <- failures_df %>%
  ggplot(aes(lifetime_at_failure)) + geom_histogram(aes(fill=failure_description), binwidth = 1) +
  facet_wrap(~part_code, ncol = 1) +
  theme_light()  + ylab("frequency") + xlab("lifetime at failure (days)") +
  ylim(0,15))
```

We already see some interesting patterns here, for example a lot of failures are recorded in the first 60 days of life for the part *K01*, followed by a second wave centered around 300 days of life. On the other hand, the "screen shutdown" seems to occur at any time of the device's life. A way to make more sense of these spiky frequencies will be to estimate failure-specific density distributions based on this information. The obtained density distributions will have two advantages:

* get a smooth distribution of when failures are likely to occur by getting rid of the uninformative spikes;
* get a vector of (relative) failure probabilities instead of frequencies. This is what we'll need for the forecasting part.

By estimating those densities at the part - failure type level, we also keep the lowest level of granularity while leveraging the whole pool of observations when an identical part is shared by multiple devices. 

We'll use a (Gaussian) Kernel density estimator to get the density functions we need. Let's first nest our dataset as follows:
```{r, echo=TRUE, warning =FALSE}
(failures_df_nested <- failures_df %>%
  select(-terminal_model) %>%
  group_by(part, part_code, failure_description) %>%
  nest())
```
where each value in the column "data" is actually a 2-columns dataframe that records for each failure the lifetime at which it occured and the total number of devices from this model that constitute the installed base. 

The following function will be applied to the nested dataset to return for each row a dataframe that contains the density estimation for the device part at hand. While the lifetime at failure will be used to estimate the density of the survival time, the install base will be usefull to weigh down these densities so that they integrate to their respective likelihood instead of one.
```{r, echo=TRUE, warning =FALSE}
get_weighted_density_df <- function(df) {
  shareofpopulation = length(df[["lifetime_at_failure"]])/max(df[["nr_terminals"]])
  kdensity <- density(df[["lifetime_at_failure"]], bw = "nrd0", kernel = "gaussian", from=0, to=3000, n =3001) 
  data.frame(
    life_length = kdensity$x,
    weighted_density = kdensity$y * shareofpopulation
  )
}
```

And here we go
```{r}
failures_densities_df_nested <- failures_df_nested %>%
  mutate(density_est = map(data, get_weighted_density_df)) %>%
  select(-data) 

failures_densities_df_nested
```

On this generated data, the kernel density estimator seems to do a pretty good job. The method has two critical advantages over parametric density estimation methods for this use case: First its flexibility makes sure it can realistically fit the heterogeneous types of empirical distributions we could potentially encounter; Second, as new data comes in, updating the density function only requires to add up the new kernels to the existing density provided that we properly weight the exising and new infomation to their respective share of the total, which makes it economical in terms of computing needs.

```{r, echo=FALSE, warning=FALSE, message = FALSE}
failures_densities_df <- unnest(failures_densities_df_nested)

p1 <- failures_densities_df %>%
  ggplot(aes(life_length, weighted_density, fill= failure_description)) +
  geom_area(alpha= 1) +
  facet_wrap(~part_code, ncol = 1) +
  ylab("weighted density") + xlab("survival length (days)") +
  theme_light()

gridExtra::grid.arrange(p0 + guides(fill = FALSE), p1, ncol = 2, nrow = 1)
```

Now that we have estimated lifetime densities for each part, we can add them up to get the lifetime densities for device A and device B.

```{r, echo=FALSE, warning =FALSE}

failure_densities_bydevice_nested <- terminals_df %>%
  left_join(failures_densities_df_nested)


failure_densities_bydevice <- failure_densities_bydevice_nested %>% 
  unnest()


p2 <- failure_densities_bydevice %>%
  ggplot(aes(life_length, weighted_density, fill= failure_description)) +
  geom_area() +
  facet_grid(terminal_model ~ part) +
  guides(fill = FALSE) +
  labs(title = "Weighted est. density", subtitle = "by terminal part", y = "weighted density", x = "survival length (days)") +
  theme_light()



p3 <- failure_densities_bydevice %>%
  ggplot(aes(life_length, weighted_density)) +
  geom_area(aes(fill = failure_description)) +
  guides(fill = FALSE) +
  labs(title = "Inferred survival time density", subtitle = "by terminal model", y = "density", x = "survival length (days)") +
  theme_light() +
  facet_wrap(~terminal_model, ncol = 1)

gridExtra::grid.arrange(p2, p3, ncol = 2, nrow = 1)
```

Obtaining these device-specific lifetime densities are not an end in itself but a necessary step to reach our final objective: forecasting the reparation flow of our electronic devices.

## Failure forecasting

Let's first illustrate the method for forecasting the failures of devices that are already sold and then discuss why the solution should be complemented with a sales planner to really provide worthwile results. 

### Forecast failures for current fleat

In the previous section, we produced part-specific density functions in lifetime at failure $\hat f_j(\tau)$. We also have weighted them down so that if we denote by $\alpha_j$ the probability that a device containing the part $j$ fails due to part $j$, the estimated lifetime at failure probability distribution of device $i$ containing the set of parts $J_i$ is:
$$\hat g_i(\tau) = \sum_{j \in J} \alpha_j \hat f_j(\tau)$$

where the weighs ensure that $\int_{\tau=1}^{T} \hat g_i(\tau) = \sum_{j \in J} \alpha_j = \alpha_i$ i.e. that the failure rate of the device $i$ is the sum of the failure rate of its parts.

Now we will use these device-specific densities in parallel with the sales data to compute the flow of failures by building on the following principle.

$$ E \Big[ Nr(failure)_t \Big] = \sum_{i=1}^n \hat g_i(\tau_i, t)$$
In other words, by adding up the device-specific weighted densities while appending them to the timeline so that $\tau = 0$ at the selling date, we'll obtain the expected number of failures at each point in time. As a first step we join the sales data with the dataframe containing our estimations to obain a dataset where each row defines a part sold at a given date together with the related estimated life-at-failure probability distribution.

```{r, warning=FALSE}
df <- data.frame(
  device_model = c(rep("A", length(parts_a)), rep("B", length(parts_b))),
  part_code = c(parts_a,parts_b)
)

forecast_nested <- sales_df %>%
  left_join(df, by = "device_model") %>%
  left_join(failure_densities_bydevice_nested, by = "part_code") %>%
  select(-c(nr_terminals,terminal_model))

datatable(forecast_nested)
```

Now we have to find a way to plug each of these probability vectors at the right place on the timeline i.e. make sure that $\tau_j=0$ where $t=date\_sold_j$** for all parts $j$, before adding them up to get the expected total number of failures throughtout time. The following function will do exactly that:

```{r}
get_timelinedf = function(start_date, df){
  
  # compute nr of zero values to append to the values vector
  nr_zero <- max(as.numeric(difftime(as.Date("2025/01/01"), as.Date(start_date),"days")) - length(df[["weighted_density"]]) + 1, 0)
  
  # create output df
  data.frame(
    timeline = seq(from = as.Date(start_date), to = as.Date("2025/01/01"), by ="days"),
    values = head(
      append(df[["weighted_density"]], rep(0,nr_zero)),
      as.numeric(difftime(as.Date("2025/01/01"), as.Date(start_date),"days")) +1 )
  )
}
```

Now we can aggregate these results per day and get our part failure forecast.

```{r}
(forecast_df <- forecast_nested %>%
  mutate(expected_failure = map2(date_sold, density_est, get_timelinedf)) %>%
  select(-density_est) %>%
  unnest() %>%
  group_by(timeline, failure_description) %>%
  summarise(values = sum(values)))
```

```{r, echo = FALSE}
forecast_df %>%
  ggplot(aes(timeline,values)) +
  geom_area(aes(fill = failure_description)) +
  labs(title = "Expected number of failures over time", y="# failures", x="") +
  theme_bw() +
  geom_vline(xintercept=Sys.Date(), linetype="dashed", color = "red")
# add vertical line at today's date
```

### Forecast failures based on sales expectations

It is clear from Figure x that we need an idea of how the device sales will evolve if we are to forecast failures in the medium run. This only requires a relatively small extention to what we already did where historical sales are complemented with a sales forecast. To a purely statistical (time series analysis) approach, I will prefer the more pragmatic way of requiring as input from business users the expected growth rate for each model's sales in the coming year(s). One could also want to combine this trend with some seasonal patterns that we can extract from historical sales data.

Suppose the sales of device A are expected to decline by 10% a year for the two next years while the sales of device should grow by 15% per year during the same period. In a real life scenario, there might also be significant seasonality in the sales figures: in this case, it turns out that clients are more likely to buy devices at the end of the accounting period. Let's first build expectations about sales for the next two years based on these two elements.

#### seasonality estimation


#### sales forecast


#### failures forecast
All we have to do now is repeat our failures forecast based not only on the currently installed base but also on our expectations about these sales for the next two years.




