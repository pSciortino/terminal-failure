---
title: "Forecasting payment terminals failure"
output:
  html_document:
    collapse: no
    df_print: paged
    toc: yes
    toc_depth: 3
    toc_float: yes
    code_folding: hide
  pdf_document:
    toc: yes
    toc_depth: '2'
bibliography: bibliography.bibtex
---
```{r setup, include=FALSE, message = FALSE}
library(tidyverse)
library(formattable)
library(DT)
library(kernhaz)
library(survPresmooth)
library(lubridate)
library(ggpubr)
library(tsbox)
library(modelr)
```
## Project summary

Say the company we work for manufactures and sells electronic devices in a B2B context while offering after sales services. In some sectors, shutdown time might be extremely costly for clients and handling devices reparation requires maintening a sufficient stock at hand. Yet, producing and storing spare parts comes with a cost which is why inventory levels should be about just high enough to cover the demand. This project explores the idea of quantifying and forecasting the occurence of device failures and therefore the future reparation needs using as input historical data on reparations (as a proxy for failure occurences) and product sales.

Literature reviews on the topic [@van2019forecasting; @dekker2013use] stress the poor suitability of traditional time series forecasting for the task of predicting spare parts demand. A first challenge is that the occurence of failures is erratic and intermitent. Another is that our expectations on the number of failures should obviously take into account the number of devices currently on the market, which we will refer to as the *installed base*, but also the expected evolution of this installed base, rather than na√Øvely extrapolating patterns observed in historical failure data. 

The method explored here will consist in three steps: First, a time-to-failure probability distribution will be estimated for each device part based on historical failure data. In a second phase, a forecast of future failures is performed based on the current state of the installed base by exploiting the fact that the expected number of part failures in time t is the sum of the individual probabilities to fail in time t for all parts in service. Finally, the exercise will be repeated in a setting where the future size and composition of the installed base can be estimated.

### Literature takeaways

The proposed method follows the literature's recommendations to think of the installed base as the driving factor for failures and therefore to exploit its current but also future state for the purpose of building expectations on future failures. Besides, it follows the subset of this literature that estimates time-to-failure probability distributions for each particular product instead of single-point estimates. What distinguishes the present attempt is the use of a non-parametric density estimation technique to build these probability distributions while generally a Weibull distribution is fitted. I also explore how statistical information can be combined with business knowledge to build pragmatic expectations on the future evolution of the installed base.

## Data generation

```{r, include = FALSE}
# Parameters ------------------------------------------------------------

### Defaults dataset generation ---------

# terminals of model A 
nr_a = 2000
failure_rate_a = 0.80
parts_a <- c("K01", "S01")

# terminals of model B
nr_b = 1000
failure_rate_b = 0.75
parts_b <- c("K02", "S01")


### Sales dataset generation ---------
startdate <- as.Date("2017/1/1")
enddate <- as.Date("2019/12/31")
sales_A <- 2000
sales_B <- 1000
```


```{r, include = FALSE, message = FALSE, warning= FALSE}
source("Generate-data.R")
```

The two datasets used as source for the analysis are generated for the purpose of this project.

### Reparation data
In total, `r nr_a` terminals of model A have been sold and `r formattable::percent(failure_rate_a,0)` were returned due to failure. More specifically, x were returned because of a screen failure and y because of a keyboard failure. 

Part-specific life-at-failure probability distributons were used to generate failures. These distributions were obtained by combining together skewed normal, truncated normal and uniform distributions so that methods based on the normality assumption or other simplifying assumptions should not be misleadingly validated.

We get:
```{r, warning =FALSE}
failures_df %>%
  head(100) #%>%
  #datatable()
```

### Sales data

Say `r sales_A` devices of model A and `r sales_B` devices of model B were sold between `r format(startdate, "%B %d %Y")` and `r format(enddate, "%B %d %Y")`.

The dataset is generated as follows: First, a sequence of consecutive dates from `r format(startdate, "%B %d %Y")` and `r format(enddate, "%B %d %Y")` is generated. Then a sample of size `r sales_A` + `r sales_B` is drawn with replacement from this sequence. Some seasonality is deliberately created by the fact that at each stochastic iteration, the relative probability for a given date in the sequence to be picked depends on its position in the calendar year.
```{r}
sales_df %>%
  head(100)
```


## Exploration and density estimation

Now let's start exploring the reparation dataset as if we didn't know its generating process. First we're interested in understanding the lifetime at which failures generally occur. Here we visualize the frequency of the devices lifetime at failure for 3 failures type and 3 different device parts.
```{r, warning=FALSE}
(p0 <- failures_df %>%
  ggplot(aes(lifetime_at_failure)) + geom_histogram(aes(fill=failure_description), binwidth = 1) +
  facet_wrap(~part_code, ncol = 1) +
  theme_light()  + ylab("frequency") + xlab("lifetime at failure (days)") +
  ylim(0,15))
```

We already see some interesting patterns here, for example a lot of failures are recorded in the first 60 days of life for the part *K01*, followed by a second wave centered around 300 days of life. On the other hand, the "screen shutdown" seems to occur at any time of the device's life. A way to make more sense of these spiky frequencies is to estimate failure-specific density distributions based on this information. The obtained density distributions will have two advantages:

* get a smooth distribution of when failures are likely to occur by getting rid of uninformative spikes;
* get a vector of (relative) failure probabilities instead of frequencies. This is what we'll need for the forecasting part.

By estimating those densities at the part - failure type level, we also keep the lowest level of granularity while leveraging the whole pool of training observations when an identical part is shared by multiple devices. 

We'll use a (Gaussian) Kernel density estimator to get the density functions we need. Let's first nest our dataset as follows:
```{r, echo=TRUE, warning =FALSE}
(failures_df_nested <- failures_df %>%
  select(-terminal_model) %>%
  group_by(part, part_code, failure_description) %>%
  nest())
```
where each value in the column "data" is actually a 2-columns dataframe that records for each failure the lifetime at which it occured and the total number of devices from the corresponding model in the installed base. 

The following function will be applied to the nested dataset to return for each row a dataframe that contains the density estimation for the device part at hand. While the lifetime at failure will be used to estimate the density of the survival time, the installed base information will be usefull to weigh down these densities so that they integrate to their respective likelihood instead of one.
```{r, echo=TRUE, warning =FALSE}
# Non-censored case
get_weighted_density_df <- function(df) {
  shareofpopulation = length(df[["lifetime_at_failure"]])/max(df[["nr_terminals"]])
  kdensity <- density(df[["lifetime_at_failure"]], bw = "nrd0", kernel = "gaussian", from=0, to=3000, n =3001) 
  data.frame(
    life_length = kdensity$x,
    weighted_density = kdensity$y * shareofpopulation
  )
}

# Censored durations case
get_weighted_presmooth_density_df  <- function(df) {
  shareofpopulation = length(df[["lifetime_at_failure"]])/max(df[["nr_terminals"]])
  kdensity <- presmooth(times = df[["lifetime_at_failure"]], status = rep(1,length(df[["lifetime_at_failure"]])), estimand = "f", x.est = seq(0,3000, by=1), bw.selec = "plug-in") 
  data.frame(
    life_length = kdensity$x.est,
    weighted_density = kdensity$estimate * shareofpopulation
  )
}

failures_densities_df_nested <- failures_df_nested %>%
  mutate(density_est = map(data, get_weighted_presmooth_density_df)) %>%
  select(-data) 
```

On this generated data, the kernel density estimator seems to do a pretty good job. Unlike a parametric density estimation method, the kernel density estimator is highly flexible which makes it able to fit the heterogeneous types of empirical distributions we could potentially encounter. That said, it has two potential drawbacks relative to an approach that assumes ex-ante a distribution for the density that is to be estimated: First, it risks *overfitting* when the number of training observations is low relative to the variance in the time-to-failure variable. This should not be a concern here given the amount of observations we have. A second possible drawback is that it is a slightly more *computation intensive* method. In a real-life scenario however, this can be mitigated by the fact that new data can be processed incrementally. Indeed, updating the density function only requires to add up the new kernels to the existing density provided that we properly weight the exising and new infomation to their respective share of total.

```{r, warning=FALSE, message = FALSE}
failures_densities_df <- unnest(failures_densities_df_nested)

p1 <- failures_densities_df %>%
  ggplot(aes(life_length, weighted_density, fill= failure_description)) +
  geom_area(alpha= 1) +
  facet_wrap(~part_code, ncol = 1) +
  ylab("weighted density") + xlab("survival length (days)") +
  theme_light()

ggpubr::ggarrange(p0, p1, common.legend = TRUE, ncol = 2, legend = "bottom")
```

Now that we have estimated a time-to-failure probability distribution for each part, we can add them up to get that of device A and B.

```{r, warning =FALSE, message = FALSE}

failure_densities_bydevice_nested <- terminals_df %>%
  left_join(failures_densities_df_nested)


failure_densities_bydevice <- failure_densities_bydevice_nested %>% 
  unnest()


p2 <- failure_densities_bydevice %>%
  ggplot(aes(life_length, weighted_density, fill= failure_description)) +
  geom_area() +
  facet_grid(terminal_model ~ part) +
  guides(fill = FALSE) +
  labs(title = "Weighted est. density", subtitle = "by terminal part", y = "weighted density", x = "survival length (days)") +
  theme_light()



p3 <- failure_densities_bydevice %>%
  ggplot(aes(life_length, weighted_density)) +
  geom_area(aes(fill = failure_description)) +
  guides(fill = FALSE) +
  labs(title = "Inferred survival time density", subtitle = "by terminal model", y = "density", x = "survival length (days)") +
  theme_light() +
  facet_wrap(~terminal_model, ncol = 1)

gridExtra::grid.arrange(p2, p3, ncol = 2, nrow = 1)
```

Obtaining these device-specific lifetime probability distributions is the first step towards forecasting the reparation flow of our electronic devices. In what follows we'll see how these vectors of probabilities can be used for this purpose.

## Failure forecasting

Let's first illustrate the method for forecasting the failures of the current installed base ‚Äì i.e. the pool of devices that have already been sold at the time the forecast is performed ‚Äì and then discuss why the solution should be complemented with sales forecasts to really provide worthwile results. 

### Static installed base setting

In the previous section, we estimate part-specific density functions, $\hat f_j(\tau)$ in their time-to-failure $\tau$. We then weight them down before showing how we can use them to infer a failure probability function for each device model. If we denote by $\alpha_j$ the probability that a device containing the part $j$ fails due to part $j$, the estimated time-to-failure probability distribution of device $i$ containing the set of parts $J_i$ is:
$$\hat g_i(\tau) = \sum_{j \in J_i} \alpha_j \hat f_j(\tau)$$

where the weights ensure that $\int_{\tau=1}^{T} \hat g_i(\tau) = \sum_{j \in J} \alpha_j = \alpha_i$ i.e. that the failure rate of the device $i$ is the sum of the failure rate of its parts.

Now we will use these device-specific densities in parallel with the sales data to compute the flow of failures by building on the following principle.

$$ E \Big[ Nr(failure)_t \Big] = \sum_{i=1}^n \hat g_i(\tau_i, t) = \sum_{i=1}^n \sum_{j \in J_i} \alpha_j \hat f_j(\tau)$$
In other words, by adding up the part-specific weighted densities while appending them to the timeline so that $\tau = 0$ at the corresponding device's selling date, we'll obtain the expected number of failures at each point in time. As a first step we join the sales data with the dataframe containing our estimations to obain a dataset where each row defines a part sold at a given date together with the related estimated life-at-failure probability distribution.

```{r, warning=FALSE}
df <- data.frame(
  device_model = c(rep("A", length(parts_a)), rep("B", length(parts_b))),
  part_code = c(parts_a,parts_b)
)

forecast_nested <- sales_df %>%
  left_join(df, by = "device_model") %>%
  left_join(failure_densities_bydevice_nested, by = "part_code") %>%
  select(-c(nr_terminals,terminal_model))

forecast_nested %>%
  head(100) 
```

Now we have to find a way to plug each of these probability vectors at the right place on the timeline i.e. make sure that $\tau_j=0$ where $t=date\_sold_j$** for all parts $j$, before adding them up to get the expected total number of failures throughtout time. The following function will do exactly that:

```{r}
get_timelinedf = function(start_date, df){
  
  # compute nr of zero values to append to the values vector
  nr_zero <- max(as.numeric(difftime(as.Date("2025/01/01"), as.Date(start_date),"days")) - length(df[["weighted_density"]]) + 1, 0)
  
  # create output df
  data.frame(
    timeline = seq(from = as.Date(start_date), to = as.Date("2025/01/01"), by ="days"),
    values = head(
      append(df[["weighted_density"]], rep(0,nr_zero)),
      as.numeric(difftime(as.Date("2025/01/01"), as.Date(start_date),"days")) +1 )
  )
}
```

Now we can aggregate these results per date and get our part failure forecast.

```{r}
forecast_df <- forecast_nested %>%
  mutate(expected_failure = map2(date_sold, density_est, get_timelinedf)) %>%
  select(-density_est) %>%
  unnest() %>%
  group_by(timeline, failure_description) %>%
  summarise(values = sum(values))
```

Note that the fact that we have kept the granularity at the failure description level enables the prediction not only of the number of failures but also of the reason for those failures. This might be important if the goal is to manage spare parts inventories or to disinguish between demand for reparation and demand for replacement.
```{r}
forecast_df %>%
  ggplot(aes(timeline,values)) +
  geom_area(aes(fill = failure_description)) +
  labs(title = "Expected number of failures over time", y="# failures", x="") +
  theme_bw() +
  geom_vline(xintercept=Sys.Date(), linetype="dashed", color = "red")
# add vertical line at today's date
```

### Evolving installed base setting

While forecasting the failures for the current installed base might be a realistic objective in the context of an end-of-life product, it is clear that the sales might continue, or even grow in the future. In this context, our model should be extended to take into account the fact that the installed base will keep evolving in terms of size and composition in the future.

#### seasonality estimation

To integrate the notion of an evolving installed base, all we need is to enrich the historical sales data with  product-specific sales forecasts. My opinion is that the main part of this forecast must come from business experts, which is why I would exclude any purely statistical (time series forecasting) approach. However, I will explore how the business knowledge of future trends might be combined with seasonal patterns that we can extract from historical sales data in a hybrid approach.

Say our sales data look like this:
```{r}
sales_df_monthly <- sales_df %>%
  group_by(device_model, yearmonth = strftime(date_sold, "%Y%m"), year = strftime(date_sold, "%Y"), month = strftime(date_sold, "%m")) %>%
  summarize(monthly_sales = n()) %>%
  arrange(year, month) 

peaks <- data.frame(
  start = paste(seq(min(sales_df_monthly$year), max(sales_df_monthly$year),1),"11", sep=""),
  end = as.character(as.numeric(paste(seq(min(sales_df_monthly$year), max(sales_df_monthly$year) ,1),"01", sep="")) + 100)
)

sales_df_monthly %>%
  filter(device_model == "A") %>%
  ggplot(aes(yearmonth, monthly_sales)) +
  geom_line(aes(group=1)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  geom_rect(data = peaks, aes(xmin = start , xmax = end, ymin = -Inf, ymax = Inf),
            inherit.aes=FALSE, alpha = 0.4, fill = c("lightblue")) +
  labs(title = "Monthly sales - historical", x = "", y = "Number of devices sold")
```

There is obviously some seasonality in the sales: client orders tend to be higher in the last two months of the year. It also seems that sales tend to increase over time. In what follows we use a multiplicative model to extract the cylical component from this series. Our goal will be to exploit it in the forecasting of future sales while letting the business figure out a plausible trend for each product's sales in the comming years.

```{r}
# focus on device A and transform the dataset in a time series object
failures_A_series <- sales_df_monthly %>%
  filter(device_model == "A")

ts_A<-ts(failures_A_series$monthly_sales, frequency =12, start= c(year(startdate), day(startdate) - 1))

# decompose time series
ts_A_model<- decompose(ts_A, "multiplicative")
#plot(ts_A_model, xlab= "")

installbase_decomposed_df <- ts_df(ts_A_model$x) %>%
  left_join(ts_df(ts_A_model$seasonal), by = "time") %>%
  rename(observed_value = value.x, seasonality = value.y)


```


#### sales forecast
Now suppose the production experts' view on the next two years is that the sales of *device A* will decrease by 20% per year while that of *device B* will increase by 20% next year and by 30% the year after.

A simple linear forecast would look like:

```{r}
# Parameter:
forecast_length = 2
yearly_growth_A = .20

# Variables:
forecast_length_days = length(seq(from = max(installbase_decomposed_df$time) + ddays(1), to = max(installbase_decomposed_df$time) + dyears(forecast_length), by = "days"))

daily_growth_A <- (1+yearly_growth_A)^(1/365)

# Baseline scenario (linear trend extrapolated)

linearmododel <- lm(observed_value~ time, installbase_decomposed_df)
start_forecast_value <- predict(linearmododel)[length(installbase_decomposed_df$observed_value)]
last_observed_value <- installbase_decomposed_df$observed_value[length(installbase_decomposed_df$observed_value)]

installedbase_forecast <- data.frame(
  time = c(installbase_decomposed_df$time,
           seq(from = max(installbase_decomposed_df$time) + ddays(1), to = max(installbase_decomposed_df$time) + dyears(forecast_length), by = "days")
  ),
  observed_value = c(installbase_decomposed_df$observed_value, rep(NA,forecast_length_days)),
  seasonality = c(installbase_decomposed_df$seasonality, rep(NA,forecast_length_days)),
  predicted_value_business = c(
    rep(NA, length(installbase_decomposed_df$observed_value) -1),
    last_observed_value,
    start_forecast_value*cumprod(rep(daily_growth_A, forecast_length_days)))
)

installedbase_forecast <- installedbase_forecast %>%
  add_predictions(linearmododel, "predicted_values_lm")

installedbase_forecast %>%
  ggplot() +
  geom_line(aes(x = time, y = observed_value)) +
  geom_line(aes(x = time, y = predicted_value_business), color = "red", linetype = "dashed")

```




And enriching it with the seasonality we estimated from past data, we can make it look like:






#### failures forecast
All we have to do now is repeat our failures forecast based not only on the currently installed base but also on our expectations about these sales for the next two years.


## References

