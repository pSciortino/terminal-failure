---
title: "Forecasting payment terminals failure"
output:
  html_document:
    collapse: no
    df_print: paged
    toc: yes
    toc_depth: 3
    toc_float: yes
    code_folding: hide
  pdf_document:
    toc: yes
    toc_depth: '2'
bibliography: bibliography.bibtex
---
```{r setup, include=FALSE, message = FALSE}
library(tidyverse)
library(formattable)
library(DT)
library(kernhaz)
library(survPresmooth)
library(lubridate)
library(ggpubr)
library(tsbox)
library(modelr)
```
## Project summary

Say the company we work for manufactures and sells electronic devices in a B2B context while offering after sales services. In some sectors, shutdown time might be extremely costly for clients and handling device reparations requires maintening a sufficient stock of spare parts at hand. Yet, producing and storing those parts comes with a cost which is why inventory levels should be about just high enough to cover the demand. This project explores the idea of quantifying and forecasting the occurence of device failures and therefore the future reparation needs using as input historical data on product sales and reparations (as a proxy for failure occurences).

Literature reviews on the topic [@van2019forecasting; @dekker2013use] stress the poor performence of traditional time series forecasting methods for the task of predicting spare parts demand. A first challenge is that the occurence of failures is erratic and intermitent. Another is that our expectations on the number of failures should obviously take into account the number of devices currently on the market, which we will refer to as the *installed base*, as well as its expected evolution, rather than naïvely extrapolating patterns observed in historical failure data. 

The method explored here consists in three steps: First, a time-to-failure probability distribution will be estimated for each device part and failure type based on historical failure data. In a second phase, a forecast of future failures is performed based on the current state of the installed base by exploiting the fact that the expected number of part failures in time t is the sum of the individual probabilities to fail in time t for all parts in service. Finally, the exercise will be repeated in a setting where the future size and composition of the installed base is estimated.

### Literature takeaways

The proposed method follows the literature's recommendations to think of the installed base as the driving factor for failures and therefore to exploit its current but also future state for the purpose of building expectations on future failures. Besides, it follows the subset of this literature that estimates time-to-failure probability distributions for each particular product instead of single-point estimates. What distinguishes the present attempt is the use of a non-parametric density estimation technique to build these probability distributions while generally a specific distribution is assumed. I also explore how statistical information can be combined with business knowledge to build pragmatic expectations on the future evolution of the installed base.

## Data generation

```{r, include = FALSE}
# Parameters ------------------------------------------------------------

### Defaults dataset generation ---------

# terminals of model A 
nr_a = 2000
failure_rate_a = 0.80
parts_a <- c("K01", "S01")

# terminals of model B
nr_b = 1000
failure_rate_b = 0.75
parts_b <- c("K02", "S01")


### Sales dataset generation ---------
startdate <- as.Date("2017/1/1")
enddate <- as.Date("2019/12/31")
sales_A <- 2000
sales_B <- 1000
```


```{r, include = FALSE, message = FALSE, warning= FALSE}
source("Generate-data.R")
```

The two datasets used as source for the analysis are generated for the purpose of this project.

### Reparation data
In total, `r nr_a` terminals of model A have been sold and `r formattable::percent(failure_rate_a,0)` were returned due to failure. More specifically, x were returned because of a screen failure and y because of a keyboard failure. 

Part-specific life-at-failure probability distributons were used to generate failures. These distributions were obtained by combining together skewed normal, truncated normal and uniform distributions so that methods based on the normality assumption or other simplifying assumptions should not be misleadingly validated.

We get:
```{r, warning =FALSE}
failures_df %>%
  head(100) #%>%
  #datatable()
```

### Sales data

Say `r sales_A` devices of model A and `r sales_B` devices of model B were sold between `r format(startdate, "%B %d %Y")` and `r format(enddate, "%B %d %Y")`.

The dataset is generated as follows: First, a sequence of consecutive dates from `r format(startdate, "%B %d %Y")` and `r format(enddate, "%B %d %Y")` is generated. Then a sample of size `r sales_A` + `r sales_B` is drawn with replacement from this sequence. Some seasonality is deliberately created by the fact that at each stochastic iteration, the relative probability for a given date in the sequence to be picked depends on its position in the calendar year.
```{r}
sales_df %>%
  head(100)
```


## Exploration and density estimation

Now let's start exploring the reparation dataset as if we didn't know its generating process. First we're interested in understanding the lifetime at which failures generally occur. Here we visualize the frequency of the devices lifetime at failure for 3 failures type and 3 different device parts.
```{r, warning=FALSE}
(p0 <- failures_df %>%
  ggplot(aes(lifetime_at_failure)) + geom_histogram(aes(fill=failure_description), binwidth = 1) +
  facet_wrap(~part_code, ncol = 1) +
  theme_light()  + ylab("frequency") + xlab("lifetime at failure (days)") +
  ylim(0,15))
```

We already see some interesting patterns here, for example a lot of failures are recorded in the first 60 days of life for the part *K01*, followed by a second wave centered around 300 days of life. On the other hand, the "screen shutdown" seems to occur at any time of the device's life. A way to make more sense of these spiky frequencies is to estimate failure-specific density distributions based on this information. The obtained density distributions will have two advantages:

* get a smooth distribution of when failures are likely to occur by getting rid of uninformative spikes;
* get a vector of (relative) failure probabilities instead of frequencies. This is what we'll need for the forecasting part.

Besides, this frequency chart as well as any histogram or distribution directly drawn from it might suffer from a bias due to the right-censored nature of the time-to-failure dataset i.e. the fact that some devices have not failed yet. Suppose the vast majority of the devices containing the parts K01 have been sold in the 50 last days. Then the peak we observed on figure **x** might simply reflect the fact 

We'll use a version of the Kernel density estimator introduced by (**cite author**) in which kernels are weighted to counter this right censoring bias. The function *presmooth* does exactly that. 
```{r, echo=TRUE, warning =FALSE}
# 1. Nest dataset so that each value in the column "data" is actually a 2-columns dataframe that records for each failure the lifetime at which it occured and the total number of devices from the corresponding model in the installed base.

(failures_df_nested <- failures_df %>%
  select(-terminal_model) %>%
  group_by(part, part_code, failure_description) %>%
  nest())
```

A function will be applied that returns for each pair (*part*; *failure type*) a probability distribution in the part's time-to-failure. While the observed lifetime at failure will be used to estimate time-to-failure densities, ratio $Nr(failed)/Nr(sold)$ will be used to weigh down these densities so that they integrate to the likelihood that corresponding failure occurs instead of one.
```{r, echo=TRUE, warning =FALSE}
# Non-censored case
get_weighted_density_df <- function(df) {
  shareofpopulation = length(df[["lifetime_at_failure"]])/max(df[["nr_terminals"]])
  kdensity <- density(df[["lifetime_at_failure"]], bw = "nrd0", kernel = "gaussian", from=0, to=3000, n =3001) 
  data.frame(
    life_length = kdensity$x,
    weighted_density = kdensity$y * shareofpopulation
  )
}

# Censored durations case
get_weighted_presmooth_density_df  <- function(df) {
  shareofpopulation = length(df[["lifetime_at_failure"]])/max(df[["nr_terminals"]])
  kdensity <- presmooth(times = df[["lifetime_at_failure"]], status = rep(1,length(df[["lifetime_at_failure"]])), estimand = "f", x.est = seq(0,3000, by=1), bw.selec = "plug-in") 
  data.frame(
    life_length = kdensity$x.est,
    weighted_density = kdensity$estimate * shareofpopulation
  )
}

failures_densities_df_nested <- failures_df_nested %>%
  mutate(density_est = map(data, get_weighted_presmooth_density_df)) %>%
  select(-data) 
```

Unlike a parametric density estimation method, the kernel density estimator is highly flexible which makes it able to fit the heterogeneous types of empirical distributions we could potentially encounter. 

<!-- That said, it has two potential drawbacks relative to an approach that assumes ex-ante a distribution for the density that is to be estimated: First, it risks *overfitting* when the number of training observations is low relative to the variance in the time-to-failure variable. This should not be a concern here given the amount of observations we have. A second possible drawback is that it is a slightly more *computation intensive* method. In the case of a classical Kernel density estimator, this could be mitigated by the fact that new data can be processed incrementally. Indeed, updating the density function only requires to add up the new kernels to the existing density provided that we properly weight the exising and new infomation to their respective share of total. In the case of this specific version of the Kernel density estimator that makes it suited for right-censored data, I'm not such such incremental processing is possible because it seems that the weights given to each kernel should all be recomputed as new data comes in. -->

```{r, warning=FALSE, message = FALSE}
failures_densities_df <- unnest(failures_densities_df_nested)

p1 <- failures_densities_df %>%
  ggplot(aes(life_length, weighted_density, fill= failure_description)) +
  geom_area(alpha= 1) +
  facet_wrap(~part_code, ncol = 1) +
  ylab("weighted density") + xlab("survival length (days)") +
  theme_light()

ggpubr::ggarrange(p0, p1, common.legend = TRUE, ncol = 2, legend = "bottom")
```

Now that we have estimated a time-to-failure probability distribution for each part, we can add them up to get that of device A and B.

```{r, warning =FALSE, message = FALSE}

failure_densities_bydevice_nested <- terminals_df %>%
  left_join(failures_densities_df_nested)


failure_densities_bydevice <- failure_densities_bydevice_nested %>% 
  unnest()


p2 <- failure_densities_bydevice %>%
  ggplot(aes(life_length, weighted_density, fill= failure_description)) +
  geom_area() +
  facet_grid(terminal_model ~ part) +
  guides(fill = FALSE) +
  labs(title = "Weighted est. density", subtitle = "by terminal part", y = "weighted density", x = "survival length (days)") +
  theme_light()



p3 <- failure_densities_bydevice %>%
  ggplot(aes(life_length, weighted_density)) +
  geom_area(aes(fill = failure_description)) +
  guides(fill = FALSE) +
  labs(title = "Inferred survival time density", subtitle = "by terminal model", y = "density", x = "survival length (days)") +
  theme_light() +
  facet_wrap(~terminal_model, ncol = 1)

gridExtra::grid.arrange(p2, p3, ncol = 2, nrow = 1)
```

Obtaining these device-specific lifetime probability distributions is the first step towards forecasting the occurence of failures. In what follows we'll see how these vectors of probabilities can be used for that purpose.

## Failure forecasting

Let's first illustrate the method for forecasting the failures of the current installed base – i.e. the pool of devices that have already been sold at the time the forecast is performed – and then discuss why the solution should be complemented with sales forecasts to really provide worthwile results. 

### Static installed base setting

In the previous section, we estimate part-specific density functions, $\hat f_j(\tau)$ in their time-to-failure $\tau$. We then weight them down before showing how we can use them to infer a failure probability function for each device model. If we denote by $\alpha_j$ the probability that a device containing the part $j$ fails due to part $j$, the estimated time-to-failure probability distribution of device $i$ containing the set of parts $J_i$ is:
$$\hat g_i(\tau) = \sum_{j \in J_i} \alpha_j \hat f_j(\tau)$$

where the weights ensure that $\int_{\tau=1}^{T} \hat g_i(\tau) = \sum_{j \in J} \alpha_j = \alpha_i$ i.e. that the failure rate of the device $i$ is the sum of the failure rate of its parts.

Now we will use these device-specific densities in parallel with the sales data to compute the flow of failures by building on the following principle.

$$ E \Big[ Nr(failure)_t \Big] = \sum_{i=1}^n \hat g_i(\tau_i, t) = \sum_{i=1}^n \sum_{j \in J_i} \alpha_j \hat f_j(\tau)$$
In other words, by adding up the part-specific weighted densities while appending them to the timeline so that $\tau = 0$ at the corresponding device's selling date, we'll obtain the expected number of failures at each point in time. As a first step we join the sales data with the dataframe containing our estimations to obain a dataset where each row defines a part sold at a given date together with the related estimated life-at-failure probability distribution.

```{r, warning=FALSE}
df <- data.frame(
  device_model = c(rep("A", length(parts_a)), rep("B", length(parts_b))),
  part_code = c(parts_a,parts_b)
)

forecast_nested <- sales_df %>%
  left_join(df, by = "device_model") %>%
  left_join(failure_densities_bydevice_nested, by = "part_code") %>%
  select(-c(nr_terminals,terminal_model))

forecast_nested %>%
  head(100) 
```

Now we have to find a way to plug each of these probability vectors at the right place on the timeline i.e. make sure that $\tau_j=0$ where $t=date\_sold_j$** for all parts $j$, before adding them up to get the expected total number of failures throughtout time. The following function will do exactly that:

```{r}
get_timelinedf = function(start_date, df){
  
  # compute nr of zero values to append to the values vector
  nr_zero <- max(as.numeric(difftime(as.Date("2025/01/01"), as.Date(start_date),"days")) - length(df[["weighted_density"]]) + 1, 0)
  
  # create output df
  data.frame(
    timeline = seq(from = as.Date(start_date), to = as.Date("2025/01/01"), by ="days"),
    values = head(
      append(df[["weighted_density"]], rep(0,nr_zero)),
      as.numeric(difftime(as.Date("2025/01/01"), as.Date(start_date),"days")) +1 )
  )
}
```

Now we can aggregate these results per date and get our part failure forecast.

```{r, warning= FALSE}
forecast_df <- forecast_nested %>%
  mutate(expected_failure = map2(date_sold, density_est, get_timelinedf)) %>%
  select(-density_est) %>%
  unnest() %>%
  group_by(timeline, failure_description) %>%
  summarise(values = sum(values))
```

Note that the fact that we have kept the granularity at the failure description level enables the prediction not only of the number of failures but also of the reason for those failures. This might be important if the goal is to manage spare parts inventories or to disinguish between demand for reparation and demand for replacement.
```{r}
forecast_df %>%
  ggplot(aes(timeline,values)) +
  geom_area(aes(fill = failure_description)) +
  labs(title = "Expected number of failures over time", y="# failures", x="") +
  theme_bw() +
  geom_vline(xintercept=Sys.Date(), linetype="dashed", color = "red")
# add vertical line at today's date
```

### Evolving installed base setting

While forecasting the failures for the current installed base might be a realistic objective in the context of an end-of-life product, it is clear that the sales might continue, or even grow in the future. In this context, our model should be extended to take into account the fact that the installed base will keep evolving in terms of size and composition in the future.

#### Sales data analysis

To integrate the notion of an evolving installed base, all we need is to enrich the historical sales data with  product-specific sales forecasts. My opinion is that the main part of this forecast must come from business experts, which is why I would exclude any purely statistical (time series forecasting) approach. However, I will explore how the business knowledge of future trends might be combined with seasonal patterns that we can extract from historical sales data in a hybrid approach.

Say our sales data look like this:
```{r}
sales_df_monthly <- sales_df %>%
  group_by(device_model, yearmonth = strftime(date_sold, "%Y%m"), year = strftime(date_sold, "%Y"), month = strftime(date_sold, "%m")) %>%
  summarize(monthly_sales = sum(sales)) %>%
  arrange(year, month) 

peaks <- data.frame(
  start = paste(seq(min(sales_df_monthly$year), max(sales_df_monthly$year),1),"10", sep=""),
  end = as.character(as.numeric(paste(seq(min(sales_df_monthly$year), max(sales_df_monthly$year) ,1),"01", sep="")) + 100)
)

sales_df_monthly %>%
  filter(device_model == "A") %>%
  ggplot(aes(yearmonth, monthly_sales)) +
  geom_line(aes(group=1)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  geom_rect(data = peaks, aes(xmin = start , xmax = end, ymin = -Inf, ymax = Inf),
            inherit.aes=FALSE, alpha = 0.4, fill = c("lightblue")) +
  labs(title = "Monthly sales - historical", x = "", y = "Number of devices sold")
```

There is obviously some seasonality in the sales: client orders tend to be higher in the last two months of the year. It also seems that sales tend to increase over time. In what follows we use a multiplicative model to extract the cylical component from this series. Our goal will be to exploit it in the forecasting of future sales while letting the business figure out a plausible trend for each product's sales in the comming years.


#### sales forecast

```{r}
# Parameter:
forecast_length_years = 2
yearly_growth_A = -.2

failures_A_series <- sales_df_monthly %>%
  filter(device_model == "A") 

# Variables:
length_observed <- length(failures_A_series$yearmonth)
forecast_length_months = forecast_length_years*12
monthly_growth_A = (1+yearly_growth_A)^(1/12)
```
Now suppose the production experts' view on the next two years is that the sales of *device A* will `r ifelse(yearly_growth_A>0, "increase", "decrease")` by 20% per year while that of *device B* will increase by 20% next year and by 30% the year after.

A simple linear forecast would look like:

```{r, warning= FALSE}
df <- failures_A_series %>%
  mutate(yearmonthday = ymd(paste(yearmonth, "01", sep = "")))

ts_A<-ts(failures_A_series$monthly_sales, frequency =12, start= c(year(startdate), day(startdate)))
ts_A_model<- decompose(ts_A, "multiplicative")


# append df to get a forecast df
last_date <- tail(df$yearmonthday , 1)
last_sales <- tail(df$monthly_sales, 1)

df2 <- data.frame(
  device_model = c(df$device_model, rep(df$device_model[1], forecast_length_months)),
  yearmonth = c(df$yearmonthday, seq(from = ymd(last_date) + months(1), length.out = forecast_length_months, by= "month")),
  sales = c(df$monthly_sales, rep(NA,forecast_length_months))
) 
  

# Get starting point
trendmodel <- lm(sales~ yearmonth, df2)
df2 <- df2 %>%
  add_predictions(trendmodel, "pred_trendmodel")
start_pred_business <- df2$pred_trendmodel[length_observed + 1]

df3 <- data.frame(
  device_model = df2$device_model,
  yearmonth = df2$yearmonth,
  sales = df2$sales,
  forecast_business = c(rep(NA, length_observed -1),
                        last_sales,
                        start_pred_business*cumprod(rep(monthly_growth_A, forecast_length_months))),
  seasonality = c(rep(1,length_observed), rep(head(as.numeric(ts_A_model$seasonal), 12),forecast_length_years))
) %>%
  mutate(month = months(yearmonth), forecast_business_seasonalized = forecast_business* seasonality)

df3 %>%
  ggplot() +
  geom_line(aes(x = yearmonth, y = sales)) +
  geom_line(aes(x = yearmonth, y = forecast_business), color = "red", linetype = "dashed") +
  labs(title = "Sales forecast - Product A", subtitle = "Business forecast", x = "")
```

And enriching it with the seasonality we estimated from past data, we can make it look like:

```{r, warning=FALSE}
df3 %>%
  ggplot() +
  geom_line(aes(x = yearmonth, y = sales)) +
  geom_line(aes(x = yearmonth, y = forecast_business_seasonalized), color = "red", linetype = "dashed") +
  labs(title = "Sales forecast - Product A", subtitle = "Enriched business forecast", x = "")
```





#### failures forecast
All we have to do now is repeat our failures forecast based not only on the currently installed base but also on our expectations about these sales for the next two years.


## References

