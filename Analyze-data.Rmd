---
title: "Forecasting payment terminals failure"
output:
  html_document:
    collapse: no
    df_print: paged
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---
```{r setup, include=FALSE}
library(tidyverse)
library(formattable)
library(DT)
```

Say your company manufactures and sells electronic devices while offering a guarantee on each product. This project explores the idea of quantifying and forecasting the occurence of such defaults and therefore the future reparation/replacement needs using as input two datasets: one of that constains the history of reparations and another that documents the decices sold.

## Dataset

```{r, include = FALSE}
# Parameters ------------------------------------------------------------

### Defaults dataset generation ---------

# terminals of model A 
nr_a = 2000
failure_rate_a = 0.80
parts_a <- c("K01", "S01")

# terminals of model B
nr_b = 1000
failure_rate_b = 0.75
parts_b <- c("K02", "S01")


### Sales dataset generation ---------
startdate <- as.Date("2015/1/1")
enddate <- as.Date("2019/12/1")
sales_A <- 100
sales_B <- 200
```


```{r, include = FALSE}
source("Generate-data.R")
```

The two datasets used as source for the analysis are generated for the purpose of this project.

### Reparation data
In total, `r nr_a` terminals of model A have been sold and `r formattable::percent(failure_rate_a,0)` were returned due to failure. More specifically, x were returned because of a screen failure and y because of a keyboard failure. (to be continued)

We get:
```{r, echo=FALSE, warning =FALSE}
failures_df %>%
  datatable()
```

### Sales data

Say `r sales_A` devices of model A and `r sales_B` devices of model B were sold between `r format(startdate, "%B %d %Y")` and `r format(enddate, "%B %d %Y")`,  where the date at which an individual device has been sold is randomly drawn from this time range. We get:
```{r}
sales_df
```


## Exploration and density estimation

Now let's start exploring the reparation dataset as if we didn't know the generation process. First we're interested in understanding the lifetime at which failures generally occur. Here we visualize the frequency of the devices lifetime at failure for 3 failures type and 3 different device parts.
```{r, echo=FALSE, warning=FALSE}
(p0 <- failures_df %>%
  ggplot(aes(lifetime_at_failure)) + geom_histogram(aes(fill=failure_description), binwidth = 1) +
  facet_wrap(~part_code, ncol = 1) +
  theme_light()  + ylab("frequency") + xlab("lifetime at failure (days)") +
  ylim(0,15))
```

We already see some interesting patterns here, for example a lot of failures are recorded in the first 60 days of life for the part *K01*, followed by a second wave centered around 300 days of life. On the other hand, the "screen shutdown" seems to occur at any time of the device's life. A way to make more sense of these spiky frequencies will be to estimate failure-specific density distributions based on this information. The obtained density distributions will have two advantages:

* get a smooth distribution of when failures are likely to occur by getting rid of the uninformative spikes;
* get a vector of (relative) failure probabilities instead of frequencies. This is what we'll need for the forecasting part.

By estimating those densities at the part - failure type level, we also keep the lowest level of granularity while leveraging the whole pool of observations when an identical part is shared by multiple devices. 

We'll use a (Gaussian) Kernel density estimator to get the density functions we need. Let's first nest our dataset as follows:
```{r, echo=TRUE, warning =FALSE}
(failures_df_nested <- failures_df %>%
  select(-terminal_model) %>%
  group_by(part, part_code, failure_description) %>%
  nest())
```
where each value in the column "data" is actually a 2-columns dataframe that records for each failure the lifetime at which it occured and the total number of devices from this model that constitute the installed base. 

The following function will be applied to the nested dataset to return for each row a dataframe that contains the density estimation for the device part at hand. While the lifetime at failure will be used to estimate the density of the survival time, the install base will be usefull to weigh down these densities so that they integrate to their respective likelihood instead of one.
```{r, echo=TRUE, warning =FALSE}
get_weighted_density_df <- function(df) {
  shareofpopulation = length(df[["lifetime_at_failure"]])/max(df[["nr_terminals"]])
  kdensity <- density(df[["lifetime_at_failure"]], bw = "nrd0", kernel = "gaussian", from=0, to=3000, n =3001) 
  data.frame(
    life_length = kdensity$x,
    weighted_density = kdensity$y * shareofpopulation
  )
}
```

And here we go
```{r}
failures_densities_df_nested <- failures_df_nested %>%
  mutate(density_est = map(data, get_weighted_density_df)) %>%
  select(-data) 

failures_densities_df_nested
```

On this generated data, the kernel density estimation seems does a pretty good job.

```{r, echo=FALSE, warning=FALSE}
failures_densities_df <- unnest(failures_densities_df_nested)

p1 <- failures_densities_df %>%
  ggplot(aes(life_length, weighted_density, fill= failure_description)) +
  geom_area(alpha= 1) +
  facet_wrap(~part_code, ncol = 1) +
  ylab("weighted density") + xlab("survival length (days)") +
  theme_light()

gridExtra::grid.arrange(p0 + guides(fill = FALSE), p1, ncol = 2, nrow = 1)
```

Now that we have estimated lifetime densities for each part, we can add them up to get the lifetime densities for device A and device B.

```{r, echo=FALSE, warning =FALSE}

failure_densities_bydevice_nested <- terminals_df %>%
  left_join(failures_densities_df_nested)


failure_densities_bydevice <- failure_densities_bydevice_nested %>% 
  unnest()


p2 <- failure_densities_bydevice %>%
  ggplot(aes(life_length, weighted_density, fill= failure_description)) +
  geom_area() +
  facet_grid(terminal_model ~ part) +
  guides(fill = FALSE) +
  labs(title = "Weighted est. density", subtitle = "by terminal part", y = "weighted density", x = "survival length (days)") +
  theme_light()



p3 <- failure_densities_bydevice %>%
  ggplot(aes(life_length, weighted_density)) +
  geom_area(aes(fill = failure_description)) +
  guides(fill = FALSE) +
  labs(title = "Inferred survival time density", subtitle = "by terminal model", y = "density", x = "survival length (days)") +
  theme_light() +
  facet_wrap(~terminal_model, ncol = 1)

gridExtra::grid.arrange(p2, p3, ncol = 2, nrow = 1)
```

Obtaining these device-specific lifetime densities are not an end in itself but a necessary step to reach our final objective: forecasting the reparation flow of our electronic devices.

## Failure forecasting

In the previous section, we produced part-specific density functions in lifetime at failure $\hat f_j(\tau)$. We also have weighted them down so that if we denote by $\alpha_j$ the probability that a device containing the part $j$ fails due to part $j$, the lifetime at failure weighted density function of device $i$ containing the set of part $J_i$ is:
$$\hat g_i(\tau) = \sum_{j \in J} \alpha_j \hat f_j(\tau)$$

where this weighting step ensures that $\int_{\tau=1}^{T} \hat g_i(\tau) = \sum_{j \in J} \alpha_j = \alpha_i$ i.e. the failure rate of the device $i$ is the sum of the failure rate of its parts.

Now we will use these device-specific densities in parallel with the sales data to compute the flow of failures by building on the following principle.

$$ E \Big[ Nr(failure)_t \Big] = \sum_{i=1}^n \hat g_i(\tau_i, t)$$
In other words, by adding up the device-specific weighted densities while appending them to the timeline so that $\tau = 1$ at the selling date, we'll obtain the expected number of failures at each point in time. Let's do try it out with a sample of 2 sold devices:

```{r, warning=FALSE}
df <- data.frame(
  device_model = c(rep("A", length(parts_a)), rep("B", length(parts_b))),
  part_code = c(parts_a,parts_b)
)

forecast_nested <- sales_df %>%
  left_join(df, by = "device_model") %>%
  left_join(failure_densities_bydevice_nested, by = "part_code") %>%
  select(-c(nr_terminals,terminal_model))

datatable(forecast_nested)
```

Now we have to find a way to plug each of these probability vectors at the right place on the timeline i.e. so that $\tau_i=0$ at *date_sold* for all $i$, before adding them up to get the expected total number of failures throughtout time. The following function will do exactly that:

```{r}
get_timelinedf = function(start_date, df){
  
  # compute nr of zero values to append to the values vector
  nr_zero <- max(as.numeric(difftime(as.Date("2025/01/01"), as.Date(start_date),"days")) - length(df[["weighted_density"]]) + 1, 0)
  
  # create output df
  data.frame(
    timeline = seq(from = as.Date(start_date), to = as.Date("2025/01/01"), by ="days"),
    values = head(
      append(df[["weighted_density"]], rep(0,nr_zero)),
      as.numeric(difftime(as.Date("2025/01/01"), as.Date(start_date),"days")) +1 )
  )
}
```

```{r}
(forecast_df <- forecast_nested %>%
  mutate(expected_failure = map2(date_sold, density_est, get_timelinedf)) %>%
  select(-density_est) %>%
  unnest() %>%
  group_by(timeline, failure_description) %>%
  summarise(values = sum(values)))
```

Let's see what we got:

```{r}
forecast_df %>%
  ggplot(aes(timeline,values)) +
  geom_area(aes(fill = failure_description)) +
  labs(title = "Expected number of failures over time", y="# failures", x="") +
  theme_bw()
# add vertical line at today's date
```






